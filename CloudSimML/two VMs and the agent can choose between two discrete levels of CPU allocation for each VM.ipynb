{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Run the Q-learning algorithm\u001b[39;00m\n\u001b[0;32m     70\u001b[0m env \u001b[38;5;241m=\u001b[39m CloudEnvironment()\n\u001b[1;32m---> 71\u001b[0m \u001b[43mq_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [2], line 39\u001b[0m, in \u001b[0;36mq_learning\u001b[1;34m(env, num_episodes)\u001b[0m\n\u001b[0;32m     37\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, num_actions)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Update Q-table\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:1242\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the parameters\n",
    "num_vms = 2\n",
    "num_cpu_levels = 3  # Number of discrete CPU allocation levels\n",
    "num_actions = num_cpu_levels ** num_vms  # Total number of possible actions\n",
    "\n",
    "# Define the Q-table\n",
    "q_table = np.zeros((num_actions, num_actions))\n",
    "\n",
    "# Define the reward function\n",
    "def reward(cpu_allocations):\n",
    "    # Dummy reward function, can be replaced with a more sophisticated one\n",
    "    total_utilization = sum(cpu_allocations)\n",
    "    return -abs(50 - total_utilization)  # Penalize deviation from 50% CPU utilization\n",
    "\n",
    "# Define the state space (discrete CPU allocation levels for each VM)\n",
    "def get_state(cpu_allocations):\n",
    "    return sum(a * num_cpu_levels**i for i, a in enumerate(cpu_allocations))\n",
    "\n",
    "# Define the Q-learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Q-learning algorithm\n",
    "def q_learning(env, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        print(\"hello\")\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Exploration-exploitation trade-off\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.randint(0, num_actions)\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update Q-table\n",
    "            q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Environment simulation (simplified)\n",
    "class CloudEnvironment:\n",
    "    def __init__(self):\n",
    "        self.vm_states = [0] * num_vms  # Initialize VM CPU allocations\n",
    "\n",
    "    def reset(self):\n",
    "        self.vm_states = [0] * num_vms\n",
    "        return get_state(self.vm_states)\n",
    "\n",
    "    def step(self, action):\n",
    "        for i in range(num_vms):\n",
    "            self.vm_states[i] = action % num_cpu_levels\n",
    "            action //= num_cpu_levels\n",
    "\n",
    "        next_state = get_state(self.vm_states)\n",
    "        reward_val = reward(self.vm_states)\n",
    "        return next_state, reward_val, False, {}\n",
    "\n",
    "# Run the Q-learning algorithm\n",
    "env = CloudEnvironment()\n",
    "q_learning(env, num_episodes=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be several reasons why the code execution is taking longer than expected:\n",
    "\n",
    "    Number of Episodes: If the number of episodes in the Q-learning loop (num_episodes) is large, it can take a considerable amount of time to complete all the episodes, especially if the environment is complex or the Q-learning algorithm needs many iterations to converge.\n",
    "\n",
    "    Complexity of the Environment: If the environment simulation or the reward function is computationally expensive, it can slow down the execution of each episode. Ensure that the environment simulation and reward calculation are as efficient as possible.\n",
    "\n",
    "    Exploration Rate (Epsilon): The exploration rate (epsilon) determines the frequency with which the agent explores new actions rather than exploiting the current best action. If epsilon is set too high, the agent will spend more time exploring and less time exploiting, leading to slower convergence.\n",
    "\n",
    "    Size of State and Action Spaces: If the state space or action space is large, it can increase the computational complexity of the Q-learning algorithm. Ensure that the state and action spaces are discretized appropriately to avoid an explosion in the number of states or actions.\n",
    "\n",
    "    Q-table Size: The size of the Q-table can also affect the execution time, especially if the state and action spaces are large. Consider using function approximation methods such as deep Q-learning (DQN) to approximate the Q-function instead of storing it explicitly in a table.\n",
    "\n",
    "    Hardware and Software Performance: The performance of your hardware (CPU, RAM, etc.) and software environment can also impact the execution time. Ensure that your hardware is capable of handling the computational load, and consider optimizing your code for better performance.\n",
    "\n",
    "You can try optimizing the code by adjusting the parameters mentioned above, simplifying the environment or reward function if possible, or using more efficient algorithms such as DQN if the problem is computationally complex. Additionally, profiling the code to identify bottlenecks and optimizing them can help improve the execution time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
